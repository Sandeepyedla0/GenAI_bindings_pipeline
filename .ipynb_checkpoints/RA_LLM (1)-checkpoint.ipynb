{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ef6c998-4f0e-48d1-8702-fa0384bd367b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d222ce67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1af19d5-be6c-4ce8-8a8a-7f533a7f2558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting together\n",
      "  Downloading together-0.2.6-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /opt/conda/lib/python3.9/site-packages (from together) (2.31.0)\n",
      "Collecting sseclient-py<2.0.0,>=1.7.2 (from together)\n",
      "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from together) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/conda/lib/python3.9/site-packages (from together) (4.66.1)\n",
      "Collecting typer<0.10.0,>=0.9.0 (from together)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.31.0->together) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.31.0->together) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.31.0->together) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.31.0->together) (2023.7.22)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.10.0,>=0.9.0->together) (8.1.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from typer<0.10.0,>=0.9.0->together) (4.5.0)\n",
      "Downloading together-0.2.6-py3-none-any.whl (39 kB)\n",
      "Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
      "\u001b[33mDEPRECATION: beakerx-base 2.2.0 has a non-standard dependency specifier ipywidgets<8pandas,>=7.5.1. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of beakerx-base or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: sseclient-py, typer, together\n",
      "Successfully installed sseclient-py-1.8.0 together-0.2.6 typer-0.9.0\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-ejxk0u37\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-ejxk0u37\n",
      "  Resolved https://github.com/huggingface/transformers to commit cc3e4781854a52cf090ffde28d884a527dab6708\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (3.13.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (0.14.1)\n",
      "Collecting safetensors>=0.3.1 (from transformers==4.36.0.dev0)\n",
      "  Downloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.36.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.36.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.36.0.dev0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.36.0.dev0) (2023.7.22)\n",
      "Downloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.36.0.dev0-py3-none-any.whl size=7923012 sha256=7bed6ad137fc07ca29f08e6d8ef622ae193b42454b35fcc0905dfb1ce349f1af\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-a7r4sh5a/wheels/14/a0/7b/8f6b25ba4110aa215fcb8d6aedd6cd4f9b9b6619190999ac2b\n",
      "Successfully built transformers\n",
      "\u001b[33mDEPRECATION: beakerx-base 2.2.0 has a non-standard dependency specifier ipywidgets<8pandas,>=7.5.1. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of beakerx-base or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: safetensors, transformers\n",
      "Successfully installed safetensors-0.4.0 transformers-4.36.0.dev0\n"
     ]
    }
   ],
   "source": [
    "!pip install together\n",
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd767f0d-d664-4193-ac40-146cf2232adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: together\n",
      "Version: 0.2.6\n",
      "Summary: Python client for Together's Cloud Platform!\n",
      "Home-page: https://github.com/togethercomputer/together\n",
      "Author: Together AI\n",
      "Author-email: support@together.ai\n",
      "License: Apache-2.0\n",
      "Location: /opt/conda/lib/python3.9/site-packages\n",
      "Requires: requests, sseclient-py, tabulate, tqdm, typer\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed71682-e63b-4a07-b8ca-85a637f0657d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36534b83-3f1b-4ef0-a7e5-31083385876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "#from human_eval.data import write_jsonl, read_problems\n",
    "import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d864cc4-0196-4bdf-8a32-94e05882034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a0a9df-11e3-48b7-860e-6c4d462f14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/opt/conda/lib/python3.9/site-packages\")\n",
    "import together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b70735-9ddf-49bb-a258-bcaac4330bb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LLM Server Checkpoint load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a510f0e-0b40-4fd9-933d-9f6bcd6c43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LlamaForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d13cc56-6f57-4daa-b1ba-3cad6d66c7fd",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Phind_LLM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3de978f9-f3b6-4f3b-9ae8-2af9065edff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phind_LLM(model_path, prompt_template):\n",
    "    \n",
    "    model = LlamaForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side='left')\n",
    "    model_path = \"Phind/Phind-CodeLlama-34B-v2\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side='left')\n",
    "    start_time =time.time()\n",
    "    inputs = tokenizer.encode_plus(prompt_template, return_tensors=\"pt\", truncation=True, max_length=1024, padding=\"max_length\")\n",
    "\n",
    "    # Input id and generation mask\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    # output generation\n",
    "    print(\"Phind-CodeLlama-34B-v2 generation in process\")\n",
    "    generate_ids = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=1024, do_sample=False, top_p=0.75, top_k=60, temperature=0.1)\n",
    "\n",
    "    # Decoding output\n",
    "    generated_code = tokenizer.decode(generate_ids[0], skip_special_tokens=True)\n",
    "    generated_code = generated_code.replace(prompt_template, \"\").split(\"\\n\\n\\n\")[0]\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time - end_time -start_time\n",
    "    print(generated_code)\n",
    "    return generated_code, execution_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9790ab4-2ee1-4d35-ae78-02e9c04bbf88",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### together.ai API implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9a9650b-0d15-413f-8151-ca14d6fe0de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "together.api_key = \"e03a7067bb512668df6dc48cffc1da1b04ff24b3639222546cffec115fd1e2fb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d25873-2eee-43de-94e9-38ea5310fb3a",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "#### model list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "242dc6b3-b039-4e6c-b6b2-fb5aeb53a91f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 models available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Austism/chronos-hermes-13b',\n",
       " 'EleutherAI/llemma_7b',\n",
       " 'EleutherAI/pythia-12b-v0',\n",
       " 'EleutherAI/pythia-1b-v0',\n",
       " 'EleutherAI/pythia-2.8b-v0',\n",
       " 'EleutherAI/pythia-6.9b',\n",
       " 'Gryphe/MythoMax-L2-13b',\n",
       " 'HuggingFaceH4/starchat-alpha',\n",
       " 'NousResearch/Nous-Hermes-13b',\n",
       " 'NousResearch/Nous-Hermes-Llama2-13b',\n",
       " 'NousResearch/Nous-Hermes-Llama2-70b',\n",
       " 'NousResearch/Nous-Hermes-llama-2-7b',\n",
       " 'NumbersStation/nsql-llama-2-7B',\n",
       " 'Open-Orca/Mistral-7B-OpenOrca',\n",
       " 'OpenAssistant/llama2-70b-oasst-sft-v10',\n",
       " 'OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5',\n",
       " 'OpenAssistant/stablelm-7b-sft-v7-epoch-3',\n",
       " 'Phind/Phind-CodeLlama-34B-Python-v1',\n",
       " 'Phind/Phind-CodeLlama-34B-v2',\n",
       " 'SG161222/Realistic_Vision_V3.0_VAE',\n",
       " 'WizardLM/WizardCoder-15B-V1.0',\n",
       " 'WizardLM/WizardCoder-Python-34B-V1.0',\n",
       " 'WizardLM/WizardLM-70B-V1.0',\n",
       " 'bigcode/starcoder',\n",
       " 'databricks/dolly-v2-12b',\n",
       " 'databricks/dolly-v2-3b',\n",
       " 'databricks/dolly-v2-7b',\n",
       " 'defog/sqlcoder',\n",
       " 'garage-bAInd/Platypus2-70B-instruct',\n",
       " 'huggyllama/llama-13b',\n",
       " 'huggyllama/llama-30b',\n",
       " 'huggyllama/llama-65b',\n",
       " 'huggyllama/llama-7b',\n",
       " 'lmsys/fastchat-t5-3b-v1.0',\n",
       " 'lmsys/vicuna-13b-v1.5-16k',\n",
       " 'lmsys/vicuna-13b-v1.5',\n",
       " 'lmsys/vicuna-7b-v1.5',\n",
       " 'mistralai/Mistral-7B-Instruct-v0.1',\n",
       " 'mistralai/Mistral-7B-v0.1',\n",
       " 'prompthero/openjourney',\n",
       " 'runwayml/stable-diffusion-v1-5',\n",
       " 'stabilityai/stable-diffusion-2-1',\n",
       " 'stabilityai/stable-diffusion-xl-base-1.0',\n",
       " 'teknium/OpenHermes-2-Mistral-7B',\n",
       " 'togethercomputer/CodeLlama-13b-Instruct',\n",
       " 'togethercomputer/CodeLlama-13b-Python',\n",
       " 'togethercomputer/CodeLlama-13b',\n",
       " 'togethercomputer/CodeLlama-34b-Instruct',\n",
       " 'togethercomputer/CodeLlama-34b-Python',\n",
       " 'togethercomputer/CodeLlama-34b',\n",
       " 'togethercomputer/CodeLlama-7b-Instruct',\n",
       " 'togethercomputer/CodeLlama-7b-Python',\n",
       " 'togethercomputer/CodeLlama-7b',\n",
       " 'togethercomputer/GPT-JT-6B-v1',\n",
       " 'togethercomputer/GPT-JT-Moderation-6B',\n",
       " 'togethercomputer/GPT-NeoXT-Chat-Base-20B',\n",
       " 'togethercomputer/Koala-13B',\n",
       " 'togethercomputer/LLaMA-2-7B-32K',\n",
       " 'togethercomputer/Llama-2-7B-32K-Instruct',\n",
       " 'togethercomputer/Pythia-Chat-Base-7B-v0.16',\n",
       " 'togethercomputer/Qwen-7B-Chat',\n",
       " 'togethercomputer/Qwen-7B',\n",
       " 'togethercomputer/RedPajama-INCITE-7B-Base',\n",
       " 'togethercomputer/RedPajama-INCITE-7B-Chat',\n",
       " 'togethercomputer/RedPajama-INCITE-7B-Instruct',\n",
       " 'togethercomputer/RedPajama-INCITE-Base-3B-v1',\n",
       " 'togethercomputer/RedPajama-INCITE-Chat-3B-v1',\n",
       " 'togethercomputer/RedPajama-INCITE-Instruct-3B-v1',\n",
       " 'togethercomputer/alpaca-7b',\n",
       " 'togethercomputer/codegen2-16B',\n",
       " 'togethercomputer/codegen2-7B',\n",
       " 'togethercomputer/falcon-40b-instruct',\n",
       " 'togethercomputer/falcon-40b',\n",
       " 'togethercomputer/falcon-7b-instruct',\n",
       " 'togethercomputer/falcon-7b',\n",
       " 'togethercomputer/guanaco-13b',\n",
       " 'togethercomputer/guanaco-65b',\n",
       " 'togethercomputer/guanaco-7b',\n",
       " 'togethercomputer/llama-2-13b-chat',\n",
       " 'togethercomputer/llama-2-13b',\n",
       " 'togethercomputer/llama-2-70b-chat',\n",
       " 'togethercomputer/llama-2-70b',\n",
       " 'togethercomputer/llama-2-7b-chat',\n",
       " 'togethercomputer/llama-2-7b',\n",
       " 'togethercomputer/mpt-30b-instruct',\n",
       " 'togethercomputer/mpt-30b',\n",
       " 'togethercomputer/mpt-7b-chat',\n",
       " 'upstage/SOLAR-0-70b-16bit',\n",
       " 'wavymulder/Analog-Diffusion']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see available models\n",
    "model_list = together.Models.list()\n",
    "\n",
    "print(f\"{len(model_list)} models available\")\n",
    "\n",
    "# print the first 10 models on the menu\n",
    "model_names = [model_dict['name'] for model_dict in model_list]\n",
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72acb053-0285-4f3e-9a45-48cd0039dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_llm_dict = {\n",
    "1 : 'Phind/Phind-CodeLlama-34B-v2',\n",
    "2 : 'Phind/Phind-CodeLlama-34B-Python-v1',\n",
    "3 : 'WizardLM/WizardCoder-Python-34B-V1.0',\n",
    "4 : 'WizardLM/WizardCoder-15B-V1.0',\n",
    "5 : 'togethercomputer/CodeLlama-13b-Instruct',\n",
    "6 : 'togethercomputer/CodeLlama-13b-Python',\n",
    "7 : 'togethercomputer/CodeLlama-13b',\n",
    "8 : 'togethercomputer/CodeLlama-34b-Instruct',\n",
    "9 : 'togethercomputer/CodeLlama-34b-Python',\n",
    "10 : 'togethercomputer/CodeLlama-34b',\n",
    "11 :'togethercomputer/CodeLlama-7b-Instruct',\n",
    "12 : 'togethercomputer/CodeLlama-7b-Python',\n",
    "13 : 'togethercomputer/CodeLlama-7b',\n",
    "}\n",
    "# print(code_llm_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b6c4e-41d6-4b92-a3d0-62bb7c619096",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "#### together.ai API Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1389397-2367-49d2-8861-4f0d9206dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def together_api(input_promt):\n",
    "    code_llm_dict = {\n",
    "    1: 'Phind/Phind-CodeLlama-34B-v2',\n",
    "    2: 'Phind/Phind-CodeLlama-34B-Python-v1',\n",
    "    3: 'WizardLM/WizardCoder-Python-34B-V1.0',\n",
    "    4: 'WizardLM/WizardCoder-15B-V1.0',\n",
    "    5: 'togethercomputer/CodeLlama-13b-Instruct',\n",
    "    6: 'togethercomputer/CodeLlama-13b-Python',\n",
    "    7: 'togethercomputer/CodeLlama-13b',\n",
    "    8: 'togethercomputer/CodeLlama-34b-Instruct',\n",
    "    9: 'togethercomputer/CodeLlama-34b-Python',\n",
    "    10: 'togethercomputer/CodeLlama-34b',\n",
    "    11: 'togethercomputer/CodeLlama-7b-Instruct',\n",
    "    12: 'togethercomputer/CodeLlama-7b-Python',\n",
    "    13: 'togethercomputer/CodeLlama-7b',\n",
    "        }\n",
    "    # Provide user options to select\n",
    "    print(\"Available options:\")\n",
    "    for key, value in code_llm_dict.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    # Get user input\n",
    "    selected_option = int(input(\"Select an option  \"))\n",
    "\n",
    "    # Check if the selected option is valid\n",
    "    if selected_option in code_llm_dict:\n",
    "        selected_LLM_model = code_llm_dict[selected_option]\n",
    "        print(f\"You selected: {selected_LLM_model}\")\n",
    "    else:\n",
    "        print(\"Invalid option. Please select a valid option.\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(f\"{selected_LLM_model}: generation in process\")\n",
    "    output = together.Complete.create(\n",
    "    prompt = input_promt, \n",
    "    model = selected_LLM_model, \n",
    "    max_tokens = 1024,\n",
    "    temperature = 0.1,\n",
    "    top_k = 60,\n",
    "    top_p = 0.75,\n",
    "    repetition_penalty = 1.1\n",
    "    #stop = ['<human>', '\\n\\n']\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    generated_code = output['output']['choices'][0]['text']\n",
    "\n",
    "    # print generated text\n",
    "    # print(output['prompt'][0]+output['output']['choices'][0]['text'])\n",
    "    # print(output['output']['choices'][0]['text'])\n",
    "    return generated_code, execution_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd425da-7a4c-45ba-8204-516a53c4caf1",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "###  prompt generation from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e10af842-9d75-44c7-a786-7f5c71ea3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promt_generation():\n",
    "    # PolyCoder_Data_Collection/Code-LMs/Data/code_generation_script/axle_projects/filepattern/src/filepattern/cpp/include/filepattern.h\n",
    "    instruction = \" Generate C++ bindings ('.cpp' file) with the Pybind11 module for a C++ class and provide only the output code without any explanation for :\"\n",
    "    \n",
    "    # instruction = \"\"\" Complete the cmake file library name is 'backend' and target_compile_definitions\n",
    "    # file_path = 'src/filepattern/cpp/bindings.cpp' :\n",
    "    # if(BUILD_PYTHON_LIB)\n",
    "    #     find_package(pybind11 CONFIG REQUIRED) \"\"\"\n",
    "\n",
    "    # Initialize a dictionary to store file contents with filenames as keys\n",
    "    file_contents = {}\n",
    "\n",
    "    # Prompt the user for the number of file paths they want to input\n",
    "    num_paths = int(input(\"Enter the number of file paths: \"))\n",
    "\n",
    "    # Loop to gather and read file contents into variables\n",
    "    for i in range(num_paths):\n",
    "        file_path = input(f\"Enter the file path {i + 1}: \")\n",
    "        file_path = Path(file_path)\n",
    "        normalized_path = os.path.normpath(file_path)\n",
    "\n",
    "    # Read the file content into a variable\n",
    "    with open(normalized_path, 'r', encoding=\"utf-8\") as source_file:\n",
    "        file_contents[f'file_{i + 1}'] = source_file.read()\n",
    "\n",
    "    # Create a prompt_template with the instruction and file contents\n",
    "    prompt_template = instruction\n",
    "    for key in file_contents:\n",
    "        prompt_template += f\" {file_contents[key]}\"\n",
    "\n",
    "    # print(f\"Generated Prompt Template: {prompt_template}\")\n",
    "\n",
    "    llm_promt = f'''{prompt_template}'''\n",
    "    return llm_promt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f59954-58d8-4a6d-9d3e-80acec52e040",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Write_Output funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51f3f723-0d79-4d0e-863e-8fdb9239cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(generated_binding,save_path):\n",
    "    # Write the content to the .cpp file\n",
    "    with open(save_path, 'w', encoding='utf-8') as cpp_file:\n",
    "        cpp_file.write(generated_binding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87f0be-c872-44d1-b360-28b15960ec48",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## LLM main funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e15dd18c-898e-4ed2-a305-d0af94acda4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select an option:\n",
      "1. Generate code from SCB NCAT LLM server (Internal server)\n",
      "2. Generate code from together.ai API\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of your choice:  1\n",
      "Enter the number of file paths:  1\n",
      "Enter the file path 1:  work/axle_projects/nyxus/src/nyx/environment.h\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'work/axle_projects/nyxus/src/nyx/environment.h'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Get user input\u001b[39;00m\n\u001b[1;32m      8\u001b[0m user_choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the number of your choice: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m generated_promt \u001b[38;5;241m=\u001b[39m \u001b[43mpromt_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# print(generated_promt)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# save_path = 'bindings.cpp'\u001b[39;00m\n\u001b[1;32m     12\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcmake.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36mpromt_generation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     normalized_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(file_path)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Read the file content into a variable\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnormalized_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m source_file:\n\u001b[1;32m     23\u001b[0m     file_contents[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m source_file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Create a prompt_template with the instruction and file contents\u001b[39;00m\n",
      "File \u001b[0;32m/opt/modules/my/conda-envs/codellm_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'work/axle_projects/nyxus/src/nyx/environment.h'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    print(\"Select an option:\")\n",
    "    print(\"1. Generate code from SCB NCAT LLM server (Internal server)\")\n",
    "    print(\"2. Generate code from together.ai API\")\n",
    "\n",
    "    # Get user input\n",
    "    user_choice = input(\"Enter the number of your choice: \")\n",
    "    generated_promt = promt_generation()\n",
    "    # print(generated_promt)\n",
    "    # save_path = 'bindings.cpp'\n",
    "    save_path = 'cmake.txt'\n",
    "\n",
    "\n",
    "    if user_choice == '1':\n",
    "        print(\"Selected : SCB NCAT LLM server\")\n",
    "\n",
    "        generated_binding,execution_time = phind_LLM(model_path, generated_promt)\n",
    "\n",
    "    elif user_choice == '2':\n",
    "        # Option 2: Code generation from together.ai API\n",
    "        print(\"Code generation from together.ai API\")\n",
    "        generated_binding, execution_time = together_api(generated_promt)\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid option. Please select a valid option (1 or 2).\")\n",
    "    write_data (generated_binding,save_path)\n",
    "\n",
    "    print(f\" Model code generation time: {execution_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dcc3c6-2178-4e2c-bdd4-fdf49b9a6da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PolyCoder_Data_Collection/Code-LMs/Data/code_generation_script/axle_projects/filepattern/src/filepattern/cpp/include/filepattern.h\n",
    "../axle_projects/nyxus/src/nyx/environment.h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codellm_env_ipy",
   "language": "python",
   "name": "test-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
