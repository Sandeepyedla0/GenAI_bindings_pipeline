{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3f457d-64d4-4e95-8641-2a265cba6b5b",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b79cf0ea-8cab-4913-a20a-2dc635068898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import extract_class\n",
    "import clang_parser\n",
    "import opensource_codellm\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161c2a6-f796-4c5a-97c1-fb491170c90b",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### CodeGenerator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a952f037-5e33-4ce9-9af0-39eb07e912e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeGenerator:\n",
    "    def __init__(self, proj_dir, output_dir):\n",
    "        self.proj_dir = proj_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.create_output_dir()\n",
    "\n",
    "    def create_output_dir(self):\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "    def write_data(self, content, save_path):\n",
    "        with open(save_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(content)\n",
    "\n",
    "    def output_destination(self,new_dir):\n",
    "        output_fname = self.proj_dir.split(\"/\")[-1]\n",
    "        return os.path.join(new_dir, f\"parsed_{output_fname}.txt\")\n",
    "\n",
    "    def generate_binding(self, user_choice, class_definition):\n",
    "        generated_promt = opensource_codellm.promt_generation(class_definition)\n",
    "        if user_choice == 1:\n",
    "            print(\"Selected : SCB NCAT LLM server\")\n",
    "            model = opensource_codellm.init_axle_models()\n",
    "            return opensource_codellm.phind_LLM(\"Phind/Phind-CodeLlama-34B-v2\", generated_promt, model)\n",
    "        elif user_choice == 2:\n",
    "            print(\"Code generation from together.ai API\")\n",
    "            return opensource_codellm.together_api(generated_promt)\n",
    "        else:\n",
    "            print(\"Invalid option. Please select a valid option (1 or 2).\")\n",
    "            return None, None\n",
    "\n",
    "    def run(self):\n",
    "        new_dir = os.path.join(self.output_dir, self.proj_dir.split('/')[-1])\n",
    "        if not os.path.exists(new_dir):\n",
    "            os.makedirs(new_dir)\n",
    "\n",
    "        output_dest = self.output_destination(new_dir)\n",
    "        class_definition, selected_class = extract_class.extract_class_main(self.proj_dir, output_dest)\n",
    "        self.write_data(class_definition, os.path.join(new_dir, f\"{self.proj_dir.split('/')[-1]}_{selected_class}_class.cpp\"))\n",
    "\n",
    "        print(\"Select an option:\")\n",
    "        print(\"1. Generate code from SCB NCAT LLM server (Internal server)\")\n",
    "        print(\"2. Generate code from together.ai API\")\n",
    "\n",
    "        #user_choice = int(input(\"Enter the number of your choice: \"))\n",
    "        # generated_binding, execution_time = self.generate_binding(user_choice, class_definition)\n",
    "        user_choice = 1\n",
    "\n",
    "        print(\"By default - Chosing together.ai API server\")\n",
    "        generated_binding, execution_time = self.generate_binding(user_choice, class_definition)\n",
    "        if generated_binding and execution_time:\n",
    "            print(f\" Model code generation time: {execution_time}\")\n",
    "            self.write_data(generated_binding, os.path.join(new_dir, f\"GenAi_{selected_class}_binding.cpp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4aff94-6e1f-48e3-86b2-57da3e181950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "project_directory :  filepattern\n",
      "ouptput_directory :  output_folder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available classes:\n",
      "1. FilesystemStream : filepattern/src/filepattern/cpp/util/fs_stream.hpp\n",
      "2. FilePattern : filepattern/src/filepattern/cpp/include/filepattern.h\n",
      "3. ExternalMergeSort : filepattern/src/filepattern/cpp/util/sort.hpp\n",
      "Selected class: FilesystemStream\n",
      "File: filepattern/src/filepattern/cpp/util/fs_stream.hpp\n",
      "Select an option:\n",
      "1. Generate code from SCB NCAT LLM server (Internal server)\n",
      "2. Generate code from together.ai API\n",
      "By default - Chosing together.ai API server\n",
      "Selected : SCB NCAT LLM server\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6828d771a1e34f718cb784a2ef71b9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2b540d99864104972a44d8ff181b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/35.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25896943b604c55aa75eb42eba4001f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e2710b6b6e449996c639ed4671f752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00007.bin:   0%|          | 0.00/9.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b4a5b7416f4d5d8288e8a81f663773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00007.bin:   0%|          | 0.00/9.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fab7a2b5af64e5ea9d2654b2ed86d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00007.bin:   0%|          | 0.00/9.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b889b3830c47d2a1a83b7de6c34fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00007.bin:   0%|          | 0.00/9.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c59f269c05d456e826212b33f10c24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00007.bin:   0%|          | 0.00/9.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d51aadf14224117875dc6c09f9750e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00006-of-00007.bin:   0%|          | 0.00/9.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f3621c428648bea25b94af010e29b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00007-of-00007.bin:   0%|          | 0.00/9.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee59c6678554bdfb35bb3e5912e4943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ffdaecad1842cf876e8a890d65de6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7d54776dc043fb9f1ff81aedf86dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/824 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad313ce0f12f4048a1c06513b3272689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a281d1c41d48b7aa7d1fe17ebdd037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/opt/modules/my/conda-envs/codellm_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/modules/my/conda-envs/codellm_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.75` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/modules/my/conda-envs/codellm_env/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `60` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phind-CodeLlama-34B-v2 generation in process\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    proj_dir = input(\"project_directory : \")\n",
    "    output_dir = input(\"ouptput_directory : \")\n",
    "    generator = CodeGenerator(proj_dir, output_dir)\n",
    "    generator.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc8d54-4889-4af2-9d3b-5ce466dfe1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codellm_env_ipy",
   "language": "python",
   "name": "test-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
